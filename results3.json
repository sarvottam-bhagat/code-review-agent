{"pr_number":2193,"repo_url":"https://github.com/mem0ai/mem0","files":[{"name":"docs/open-source/quickstart.mdx","issues":[],"patch":"@@ -245,6 +245,106 @@ m.delete_all(user_id=\"alice\")\n m.reset() # Reset all memories\n ```\n \n+## Configuration Parameters\n+\n+Mem0 offers extensive configuration options to customize its behavior according to your needs. These configurations span across different components like vector stores, language models, embedders, and graph stores.\n+\n+<AccordionGroup>\n+<Accordion title=\"Vector Store Configuration\">\n+| Parameter    | Description                     | Default     |\n+|-------------|---------------------------------|-------------|\n+| `provider`   | Vector store provider (e.g., \"qdrant\") | \"qdrant\"   |\n+| `host`       | Host address                    | \"localhost\" |\n+| `port`       | Port number                     | 6333       |\n+</Accordion>\n+\n+<Accordion title=\"LLM Configuration\">\n+| Parameter              | Description                                   | Provider          |\n+|-----------------------|-----------------------------------------------|-------------------|\n+| `provider`            | LLM provider (e.g., \"openai\", \"anthropic\")    | All              |\n+| `model`               | Model to use                                  | All              |\n+| `temperature`         | Temperature of the model                      | All              |\n+| `api_key`             | API key to use                                | All              |\n+| `max_tokens`          | Tokens to generate                            | All              |\n+| `top_p`               | Probability threshold for nucleus sampling    | All              |\n+| `top_k`               | Number of highest probability tokens to keep  | All              |\n+| `http_client_proxies` | Allow proxy server settings                   | AzureOpenAI      |\n+| `models`              | List of models                                | Openrouter       |\n+| `route`               | Routing strategy                              | Openrouter       |\n+| `openrouter_base_url` | Base URL for Openrouter API                  | Openrouter       |\n+| `site_url`            | Site URL                                      | Openrouter       |\n+| `app_name`            | Application name                              | Openrouter       |\n+| `ollama_base_url`     | Base URL for Ollama API                      | Ollama           |\n+| `openai_base_url`     | Base URL for OpenAI API                      | OpenAI           |\n+| `azure_kwargs`        | Azure LLM args for initialization            | AzureOpenAI      |\n+| `deepseek_base_url`   | Base URL for DeepSeek API                    | DeepSeek         |\n+</Accordion>\n+\n+<Accordion title=\"Embedder Configuration\">\n+| Parameter    | Description                     | Default                      |\n+|-------------|---------------------------------|------------------------------|\n+| `provider`   | Embedding provider              | \"openai\"                     |\n+| `model`      | Embedding model to use          | \"text-embedding-3-small\"     |\n+| `api_key`    | API key for embedding service   | None                        |\n+</Accordion>\n+\n+<Accordion title=\"Graph Store Configuration\">\n+| Parameter    | Description                     | Default     |\n+|-------------|---------------------------------|-------------|\n+| `provider`   | Graph store provider (e.g., \"neo4j\") | \"neo4j\"    |\n+| `url`        | Connection URL                  | None        |\n+| `username`   | Authentication username         | None        |\n+| `password`   | Authentication password         | None        |\n+</Accordion>\n+\n+<Accordion title=\"General Configuration\">\n+| Parameter         | Description                          | Default                    |\n+|------------------|--------------------------------------|----------------------------|\n+| `history_db_path` | Path to the history database         | \"{mem0_dir}/history.db\"    |\n+| `version`         | API version                          | \"v1.0\"                     |\n+| `custom_prompt`   | Custom prompt for memory processing  | None                       |\n+</Accordion>\n+\n+<Accordion title=\"Complete Configuration Example\">\n+```python\n+config = {\n+    \"vector_store\": {\n+        \"provider\": \"qdrant\",\n+        \"config\": {\n+            \"host\": \"localhost\",\n+            \"port\": 6333\n+        }\n+    },\n+    \"llm\": {\n+        \"provider\": \"openai\",\n+        \"config\": {\n+            \"api_key\": \"your-api-key\",\n+            \"model\": \"gpt-4\"\n+        }\n+    },\n+    \"embedder\": {\n+        \"provider\": \"openai\",\n+        \"config\": {\n+            \"api_key\": \"your-api-key\",\n+            \"model\": \"text-embedding-3-small\"\n+        }\n+    },\n+    \"graph_store\": {\n+        \"provider\": \"neo4j\",\n+        \"config\": {\n+            \"url\": \"neo4j+s://your-instance\",\n+            \"username\": \"neo4j\",\n+            \"password\": \"password\"\n+        }\n+    },\n+    \"history_db_path\": \"/path/to/history.db\",\n+    \"version\": \"v1.1\",\n+    \"custom_prompt\": \"Optional custom prompt for memory processing\"\n+}\n+```\n+</Accordion>\n+</AccordionGroup>\n+\n ## Run Mem0 Locally\n \n Please refer to the example [Mem0 with Ollama](../examples/mem0-with-ollama) to run Mem0 locally.\n@@ -384,4 +484,4 @@ Please make sure your code follows our coding conventions and is well-documented\n \n If you have any questions, please feel free to reach out to us using one of the following methods:\n \n-<Snippet file=\"get-help.mdx\" />\n+<Snippet file=\"get-help.mdx\" />\n\\ No newline at end of file"}],"summary":{"total_files":1,"total_issues":0,"critical_issues":0,"issues_by_type":{"style":0,"bug":0,"performance":0,"best_practice":0}}}